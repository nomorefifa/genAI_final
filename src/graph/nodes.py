"""
Nodes - LangGraphì˜ Node í•¨ìˆ˜ë“¤

- llm_node: LLMì—ê²Œ Thought + Actionì„ ê²°ì •í•˜ë„ë¡ ìš”ì²­
- tool_node: Action(Tool í˜¸ì¶œ)ì„ ì‹¤í–‰í•˜ê³  Observation ë°˜í™˜
"""

import os
import json
from typing import Dict, Any, List
from openai import OpenAI
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.graph.state import AgentState
from src.tools.tool_registry import get_tool_specs, execute_tool, register_default_tools

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
MODEL = "gpt-4o-mini"

# ToolRegistry ì´ˆê¸°í™” (Memory Read íŒŒì´í”„ë¼ì¸ì—ì„œ ì‚¬ìš©)
_tool_registry = None

def get_tool_registry():
    """ToolRegistry ì‹±ê¸€í†¤ ê°€ì ¸ì˜¤ê¸°"""
    global _tool_registry
    if _tool_registry is None:
        _tool_registry = register_default_tools()
    return _tool_registry

# =============================================================================
# System Prompt - ReAct íŒ¨í„´ ê°€ì´ë“œ
# =============================================================================

SYSTEM_PROMPT = """\
You are an AI assistant that uses tools (functions), RAG, and memory.

# High-level behavior
- Be helpful, honest, and concise.
- Answer primarily in Korean unless the user clearly wants another language.
- Think step by step internally, but do NOT expose chain-of-thought.
- When tools are available and helpful, call them instead of guessing.

# Tools and ReAct-style behavior
- You may call tools such as:
  - read_memory: to recall important past information about the user or past sessions.
  - write_memory: to store new, useful information about the user or this conversation.
  - search_documents: to search course materials (RAG with Reranking for LangGraph, ReAct, Function Calling, etc.).
  - google_search: to search the web for latest information.
  - calculator: for arithmetic operations.
  - get_time: to check current time in a specific timezone.

- Use tools when:
  - You lack required factual details.
  - You need to recall prior user preferences, past discussions, or long-term context.
  - You need domain knowledge stored in a vector database or document store.
- After receiving a tool result, incorporate it into your reasoning and produce a final answer.

# Memory usage guidelines
- Memory is not magic; you must explicitly call `read_memory` or `write_memory` to use it.
- Call `read_memory` when:
  - The user refers to "ì§€ë‚œ ë²ˆ", "ì´ì „ì— ë§í–ˆë“¯ì´", "ì €ë²ˆì— ë§Œë“¤ë˜ ì½”ë“œ" ë“± ê³¼ê±° ë‚´ìš©.
  - The answer clearly depends on the user's preferences, profile, or long-term history.
- Call `write_memory` when:
  - The user shares stable personal preferences (e.g., ì¢‹ì•„í•˜ëŠ” ìŠ¤íƒ€ì¼, ì„ í˜¸ ì˜µì…˜).
  - The user states long-term goals, ongoing projects, or recurring topics.
  - The user corrects you or provides important facts that will be useful later.
- Do NOT write memory for:
  - Short-lived, one-off facts (ì˜ˆ: ì˜¤ëŠ˜ ì ì‹¬ ë©”ë‰´).
  - Extremely detailed logs that are unlikely to be reused.
  - Sensitive personal data, unless the user explicitly requests you to remember it.

# RAG usage guidelines
- Call search_documents when:
  - The user asks for factual information from course materials.
  - You need detailed or authoritative content about LangGraph, ReAct, RAG, Memory, Function Calling, etc.
- When you get retrieved documents, read them and synthesize a clear, concise answer.

# Answer style
- Default: Korean, ì¹œì ˆí•˜ì§€ë§Œ êµ°ë”ë”ê¸° ì—†ì´.
- Provide structure (ë²ˆí˜¸, ì†Œì œëª©) for teaching/explaining technical concepts.
- If the user is building a system or code, show step-by-step reasoning in high level,
  but do NOT output low-level hidden chain-of-thought or internal scratch work.

# Safety
- If a user asks you to perform unsafe, illegal, or harmful actions, politely refuse.
- If you're unsure, say so and explain what additional information would be needed.
"""


# =============================================================================
# Helper: tool_callsë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
# =============================================================================

def convert_tool_calls_to_dict(tool_calls) -> List[Dict]:
    """OpenAI tool_callsë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
    if not tool_calls:
        return None
    
    result = []
    for tc in tool_calls:
        if isinstance(tc, dict):
            # ì´ë¯¸ ë”•ì…”ë„ˆë¦¬ë©´ ê·¸ëŒ€ë¡œ
            result.append(tc)
        else:
            # ê°ì²´ë©´ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
            result.append({
                "id": getattr(tc, "id", ""),
                "type": "function",
                "function": {
                    "name": getattr(tc.function, "name", ""),
                    "arguments": getattr(tc.function, "arguments", "")
                }
            })
    return result


# =============================================================================
# Helper: ë©”ì‹œì§€ë¥¼ OpenAI í˜•ì‹ìœ¼ë¡œ ë³€í™˜
# =============================================================================

def convert_messages_to_openai_format(messages: List) -> List[Dict[str, Any]]:
    """
    LangGraph ë©”ì‹œì§€ë¥¼ OpenAI API í˜•ì‹ìœ¼ë¡œ ë³€í™˜

    Args:
        messages: LangGraphì˜ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ (ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” ê°ì²´)

    Returns:
        OpenAI API í˜•ì‹ì˜ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
    """
    converted = []

    for idx, msg in enumerate(messages):
        # ì´ë¯¸ ë”•ì…”ë„ˆë¦¬ë©´ ê²€ì¦ í›„ ì‚¬ìš©
        if isinstance(msg, dict):
            # ë”•ì…”ë„ˆë¦¬ê°€ ì´ë¯¸ ì˜¬ë°”ë¥¸ OpenAI í˜•ì‹ì¸ì§€ í™•ì¸
            # roleì´ ìˆê³ , toolì¸ ê²½ìš° tool_call_idê°€ ìˆì–´ì•¼ í•¨
            if msg.get("role") == "tool":
                # tool ë©”ì‹œì§€ëŠ” tool_call_idì™€ contentê°€ í•„ìˆ˜
                if "tool_call_id" not in msg or "content" not in msg:
                    print(f"âš ï¸ Warning: Incomplete tool message at index {idx}: {msg.get('name', 'unknown')}")
                    # ìŠ¤í‚µí•˜ì§€ ì•Šê³  ê²½ê³ ë§Œ ì¶œë ¥
                # tool ë©”ì‹œì§€ëŠ” ë¬´ì¡°ê±´ ì¶”ê°€ (ìŠ¤í‚µí•˜ë©´ OpenAI API ì—ëŸ¬ ë°œìƒ)
                converted.append(msg)
            else:
                # ë‹¤ë¥¸ ë©”ì‹œì§€ëŠ” ê·¸ëŒ€ë¡œ ì¶”ê°€
                converted.append(msg)

        # ê°ì²´ë©´ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        else:
            # LangGraph ë©”ì‹œì§€ íƒ€ì… ë§¤í•‘
            role_map = {
                "human": "user",
                "ai": "assistant",
                "tool": "tool",
                "system": "system"
            }

            msg_type = getattr(msg, "type", "human")
            role = role_map.get(msg_type, "user")

            msg_dict = {
                "role": role,
                "content": getattr(msg, "content", "") or ""
            }

            # tool_callsê°€ ìˆìœ¼ë©´ ì¶”ê°€ (OpenAI í˜•ì‹ìœ¼ë¡œ ì •ê·œí™”)
            tool_calls = getattr(msg, "tool_calls", None)
            if tool_calls:
                openai_tool_calls = []
                for tc in tool_calls:
                    if isinstance(tc, dict):
                        # LangGraph í˜•ì‹: {name, args, id, type: "tool_call"}
                        if "name" in tc and "args" in tc:
                            openai_tool_calls.append({
                                "id": tc.get("id", ""),
                                "type": "function",  # â† OpenAIëŠ” "function"ë§Œ ë°›ìŒ!
                                "function": {
                                    "name": tc["name"],
                                    "arguments": json.dumps(tc["args"], ensure_ascii=False)  # ë‹¤ì‹œ JSON ë¬¸ìì—´ë¡œ
                                }
                            })
                        # ì´ë¯¸ OpenAI í˜•ì‹
                        elif "function" in tc:
                            openai_tool_calls.append(tc)
                    else:
                        # ê°ì²´ í˜•íƒœ
                        openai_tool_calls.append({
                            "id": getattr(tc, "id", ""),
                            "type": "function",
                            "function": {
                                "name": getattr(tc.function, "name", ""),
                                "arguments": getattr(tc.function, "arguments", "")
                            }
                        })

                if openai_tool_calls:
                    msg_dict["tool_calls"] = openai_tool_calls

            # tool ë©”ì‹œì§€ì¸ ê²½ìš° tool_call_id ì¶”ê°€
            if role == "tool":
                if hasattr(msg, "tool_call_id") and msg.tool_call_id:
                    msg_dict["tool_call_id"] = msg.tool_call_id

                # nameë„ ì¶”ê°€
                if hasattr(msg, "name") and msg.name:
                    msg_dict["name"] = msg.name

            converted.append(msg_dict)

    return converted


# =============================================================================
# Memory Read íŒŒì´í”„ë¼ì¸ - ìë™ ë©”ëª¨ë¦¬ ê²€ìƒ‰
# =============================================================================

def execute_memory_read_pipeline(openai_messages: List[Dict[str, Any]]) -> str:
    """
    ì „ì²˜ë¦¬ ë‹¨ê³„ì—ì„œ ìë™ìœ¼ë¡œ ê´€ë ¨ ë©”ëª¨ë¦¬ë¥¼ ê²€ìƒ‰í•˜ëŠ” íŒŒì´í”„ë¼ì¸

    Args:
        openai_messages: OpenAI í˜•ì‹ì˜ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸

    Returns:
        ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ (ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´)
    """

    # 1. ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ì¶œ
    last_user_msg = None
    for msg in reversed(openai_messages):
        if msg.get("role") == "user":
            last_user_msg = msg.get("content", "")
            break

    if not last_user_msg:
        return ""

    # 2. ê³¼ê±° ì°¸ì¡° í‚¤ì›Œë“œ ê°ì§€
    past_keywords = [
        "ì§€ë‚œë²ˆ", "ì§€ë‚œ ë²ˆ", "ì €ë²ˆ", "ì´ì „", "ì „ì—",
        "ì•„ê¹Œ", "ë°©ê¸ˆ", "ì „ì— ë§í–ˆë“¯", "ë§í–ˆë˜", "ì–˜ê¸°í–ˆë˜"
    ]

    has_past_reference = any(keyword in last_user_msg for keyword in past_keywords)

    if not has_past_reference:
        return ""  # ê³¼ê±° ì°¸ì¡°ê°€ ì—†ìœ¼ë©´ ê²€ìƒ‰ ì•ˆ í•¨

    # 3. read_memory ìë™ í˜¸ì¶œ
    try:
        registry = get_tool_registry()

        print(f"\nğŸ’¾ Memory Read íŒŒì´í”„ë¼ì¸ ì‹¤í–‰:")
        print(f"   ğŸ“ Query: {last_user_msg[:50]}...")

        memory_result = registry.call("read_memory", {
            "query": last_user_msg,
            "memory_type": "all",
            "top_k": 3
        })

        memory_data = json.loads(memory_result)

        if not memory_data.get("success"):
            return ""

        memories = memory_data.get("memories", [])

        if not memories:
            print(f"   â„¹ï¸  ê´€ë ¨ ê¸°ì–µì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return ""

        # 4. ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        memory_context = "\n\n" + "="*60 + "\n"
        memory_context += "ğŸ“š ê´€ë ¨ ê¸°ì–µ (ìë™ ê²€ìƒ‰ë¨)\n"
        memory_context += "="*60 + "\n\n"

        for i, mem in enumerate(memories, 1):
            memory_context += f"{i}. [{mem.get('memory_type', 'unknown')}] "
            memory_context += f"(ì¤‘ìš”ë„: {mem.get('importance', 0)}/5)\n"
            memory_context += f"   {mem.get('content', '')}\n"
            memory_context += f"   (ìœ ì‚¬ë„: {mem.get('similarity', 0):.3f})\n\n"

        print(f"   âœ… {len(memories)}ê°œì˜ ê´€ë ¨ ê¸°ì–µì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")

        return memory_context

    except Exception as e:
        print(f"   âš ï¸ Memory Read íŒŒì´í”„ë¼ì¸ ì—ëŸ¬: {e}")
        return ""


# =============================================================================
# LLM Node - Thought + Action ê²°ì •
# =============================================================================

def llm_node(state: AgentState) -> Dict[str, Any]:
    """
    LLMì—ê²Œ í˜„ì¬ ìƒíƒœë¥¼ ì „ë‹¬í•˜ê³  ë‹¤ìŒ í–‰ë™ì„ ê²°ì •í•˜ë„ë¡ ìš”ì²­

    Returns:
        - messages: LLMì˜ ì‘ë‹µ ë©”ì‹œì§€ (tool_calls í¬í•¨ ê°€ëŠ¥)
        - loop_count: í˜„ì¬ ë£¨í”„ ì¹´ìš´íŠ¸ +1
    """
    messages = state["messages"]
    loop_count = state.get("loop_count", 0)

    # ë©”ì‹œì§€ë¥¼ OpenAI í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    openai_messages = convert_messages_to_openai_format(messages)

    # ===== Memory Read íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ì „ì²˜ë¦¬) =====
    memory_context = execute_memory_read_pipeline(openai_messages)

    # System Promptì— ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
    system_prompt_with_memory = SYSTEM_PROMPT
    if memory_context:
        system_prompt_with_memory = SYSTEM_PROMPT + memory_context

    # OpenAI API í˜¸ì¶œ
    response = client.chat.completions.create(
        model=MODEL,
        messages=[
            {"role": "system", "content": system_prompt_with_memory},
            *openai_messages
        ],
        tools=get_tool_specs(),
        tool_choice="auto"
    )
    
    msg = response.choices[0].message
    
    # ë””ë²„ê¹… ì¶œë ¥
    print(f"\n{'='*60}")
    print(f"ğŸ¤– LLM Node (ë£¨í”„ {loop_count + 1})")
    print(f"{'='*60}")
    
    if msg.content:
        print(f"ğŸ’­ Thought: {msg.content[:200]}...")
    
    if msg.tool_calls:
        print(f"ğŸ”§ Actions:")
        for tc in msg.tool_calls:
            print(f"   - {tc.function.name}({tc.function.arguments})")
    
    # ë©”ì‹œì§€ êµ¬ì„±
    new_message = {
        "role": "assistant",
        "content": msg.content or "",
    }
    
    if msg.tool_calls:
        # OpenAI tool_callsë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        new_message["tool_calls"] = convert_tool_calls_to_dict(msg.tool_calls)
    
    return {
        "messages": [new_message],
        "loop_count": loop_count + 1
    }


# =============================================================================
# Tool Node - Action ì‹¤í–‰ + Observation
# =============================================================================

def tool_node(state: AgentState) -> Dict[str, Any]:
    """
    LLMì´ ìš”ì²­í•œ Toolì„ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜
    
    Returns:
        - messages: Tool ì‹¤í–‰ ê²°ê³¼ ë©”ì‹œì§€ë“¤
    """
    messages = state["messages"]
    last_message = messages[-1]
    
    print(f"\n{'='*60}")
    print(f"ğŸ”¨ Tool Node - Executing Actions")
    print(f"{'='*60}")
    
    # last_messageë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
    if not isinstance(last_message, dict):
        role_map = {
            "human": "user",
            "ai": "assistant",
            "tool": "tool",
            "system": "system"
        }
        msg_type = getattr(last_message, "type", "ai")
        role = role_map.get(msg_type, "assistant")
        
        last_message = {
            "role": role,
            "content": getattr(last_message, "content", "") or "",
            "tool_calls": convert_tool_calls_to_dict(getattr(last_message, "tool_calls", None))
        }
    
    # tool_callsê°€ ì—†ìœ¼ë©´ ì—ëŸ¬
    tool_calls = last_message.get("tool_calls")
    if not tool_calls:
        print("âš ï¸ Warning: tool_node called but no tool_calls found")
        return {"messages": []}
    
    # ê° Tool ì‹¤í–‰
    tool_messages = []
    
    for tool_call in tool_calls:
        # LangGraph í˜•ì‹ì¸ì§€ OpenAI í˜•ì‹ì¸ì§€ í™•ì¸
        if "name" in tool_call and "args" in tool_call:
            # LangGraph í˜•ì‹: {name, args, id, type}
            tool_name = tool_call["name"]
            arguments = tool_call["args"]  # ì´ë¯¸ ë”•ì…”ë„ˆë¦¬
            tool_call_id = tool_call["id"]
        elif "function" in tool_call:
            # OpenAI í˜•ì‹: {id, type, function: {name, arguments}}
            tool_name = tool_call["function"]["name"]
            arguments = json.loads(tool_call["function"]["arguments"] or "{}")
            tool_call_id = tool_call["id"]
        else:
            # ì•Œ ìˆ˜ ì—†ëŠ” í˜•ì‹ - ê°ì²´ë¡œ ê°•ì œ ë³€í™˜ ì‹œë„
            print(f"âš ï¸ Warning: Unknown tool_call format, attempting conversion")
            tool_name = getattr(tool_call.function, "name", "") if hasattr(tool_call, "function") else ""
            arguments = json.loads(getattr(tool_call.function, "arguments", "{}") if hasattr(tool_call, "function") else "{}")
            tool_call_id = getattr(tool_call, "id", "")
        
        print(f"\nğŸ” Executing: {tool_name}")
        print(f"   Arguments: {arguments}")
        
        try:
            # Tool ì‹¤í–‰
            tool_output = execute_tool(tool_name, arguments)
            print(f"   âœ… Success")
            
            # ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸° (ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ì„œ í‘œì‹œ)
            preview = tool_output[:200] + "..." if len(tool_output) > 200 else tool_output
            print(f"   ğŸ“Š Result Preview: {preview}")
            
        except Exception as e:
            print(f"   âŒ Error: {str(e)}")
            tool_output = json.dumps({
                "error": f"Tool execution failed: {str(e)}",
                "tool_name": tool_name,
                "arguments": arguments
            }, ensure_ascii=False)
        
        # Tool ë©”ì‹œì§€ ì¶”ê°€
        tool_messages.append({
            "role": "tool",
            "tool_call_id": tool_call_id,
            "name": tool_name,
            "content": tool_output
        })
    
    return {"messages": tool_messages}


# =============================================================================
# Helper: Should Continue ê²°ì • í•¨ìˆ˜
# =============================================================================

def should_continue(state: AgentState) -> str:
    """
    ë‹¤ìŒì— ì–´ë””ë¡œ ê°ˆì§€ ê²°ì •í•˜ëŠ” í•¨ìˆ˜
    
    Returns:
        - "tools": Tool ì‹¤í–‰ì´ í•„ìš”í•¨ (tool_callsê°€ ìˆìŒ)
        - "end": ìµœì¢… ë‹µë³€ ì™„ë£Œ (tool_callsê°€ ì—†ìŒ)
    """
    messages = state["messages"]
    last_message = messages[-1]
    loop_count = state.get("loop_count", 0)
    
    # ë¬´í•œ ë£¨í”„ ë°©ì§€ (ìµœëŒ€ 10ë²ˆ)
    MAX_LOOPS = 10
    if loop_count >= MAX_LOOPS:
        print(f"\nâš ï¸ Max loops ({MAX_LOOPS}) reached. Forcing end.")
        return "end"
    
    # last_messageë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
    if not isinstance(last_message, dict):
        role_map = {
            "human": "user",
            "ai": "assistant",
            "tool": "tool",
            "system": "system"
        }
        msg_type = getattr(last_message, "type", "ai")
        role = role_map.get(msg_type, "assistant")
        
        last_message = {
            "role": role,
            "content": getattr(last_message, "content", "") or "",
            "tool_calls": convert_tool_calls_to_dict(getattr(last_message, "tool_calls", None))
        }
    
    # tool_callsê°€ ìˆìœ¼ë©´ Tool Nodeë¡œ
    if last_message.get("tool_calls"):
        return "tools"
    
    # ì—†ìœ¼ë©´ ì¢…ë£Œ
    return "end"